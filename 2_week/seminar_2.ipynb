{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7zZBtixjCebK",
        "ktAtfesGrSDb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snv-ds/NLP_course/blob/master/week2/seminar_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1oBo3Azdy7L"
      },
      "source": [
        "# Seminar 2 (Word representation and CNN for text classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gf7_dLEfg71"
      },
      "source": [
        "## Purpose of the lesson"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCFQ8ClNfwEd"
      },
      "source": [
        "In this lesson, we will touch on two topics: \n",
        "- data research through training your language model and studying the vector representation of words from data\n",
        "- solving practical problem of classifying the sentiment of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36kSDfDqjO04"
      },
      "source": [
        "Practical skills, that you will learn:\n",
        "- training model, that handle distributional semantics over your training dataset;\n",
        "- dimension reduction to more effectively handle vectors and futher use simplier and more light weighted machine learning models;\n",
        "- representation of vectors on 2 dimensional graphics to find some structure in data, relative positions of vectors and similar meanings of words;\n",
        "- downloading open-source datasets for sentiment analysis;\n",
        "- downloading pretrained on large corpus weights;\n",
        "- initialize pretrained weights to your model and use them to get better quality of model;\n",
        "- writing custom class to process data for your model;\n",
        "- learn how to use CNN for text classification;\n",
        "- write simple training/validation loop to train your model;\n",
        "- some kind of transfer learning to solve text classification task for faster, using less data and more effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46IFrLhed7vo"
      },
      "source": [
        "## PART 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtY20Aavhous"
      },
      "source": [
        "In Part 1 we use texts, but you can also use other data to train vector represention of your tokens.<br> For example: each nodes in graph can be represented as random walk over neighbour nodes. After learning vector representation of your nodes you can cluster your data. It is computational more effective, because all graph clustering algorigtms have O(n**2) complexity and harder.<br>\n",
        "Or you can represent your customers as chain of their transactions and then learn their representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDs0xRMKA8xv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from typing import Union, List, Tuple, Dict, Set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAZEp7bkfmJc"
      },
      "source": [
        "### Vector normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KklCrHsm8PH"
      },
      "source": [
        "When you work with classical ML problems you need to preprocess you features to equal scale. It helps models to faster converge and get more accurate solution.<br>\n",
        "It`s all applicable for texts and their vector representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwfxADindXY"
      },
      "source": [
        "Lets get some simple vectors. They are similar, but have pretty different lenghts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e8dwyrEBDau"
      },
      "source": [
        "vectors = np.array([[ 7.,  8.,  5.,  5.],\n",
        "                    [ 9.,  8., 10.,  8.],\n",
        "                    [10.,  8.,  8.,  6.]])\n",
        "vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fDxIlzxB6I9"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN3TsNzICKKZ"
      },
      "source": [
        "#### Task 1\n",
        "Write function that get two vectors as input, normalize them to get length equal 1 and return them as torch.tensor for futher work in our Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0nqalDjoSme"
      },
      "source": [
        "We will use whis function when we will preprocess all vectors, that we get for our model. Especially if we get vectors from outside."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zZBtixjCebK"
      },
      "source": [
        "##### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA351qU5CjOM"
      },
      "source": [
        "# 1. use torch.nn.functional module to normalize (there are normalize function)\n",
        "# 2. don`t forget to cast types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YZAuQmyCeWA"
      },
      "source": [
        "##### continue work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WuZLLzNC02C"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def normalize_vectors(vectors: Union[List[Union[int, float]],\n",
        "                                     np.array,\n",
        "                                     torch.tensor]) -> torch.tensor:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTLwZBNtDghS"
      },
      "source": [
        "normed_vectors = normalize_vectors(vectors)\n",
        "\n",
        "assert torch.norm(normed_vectors, dim=1).sum().item() == 3\n",
        "assert normed_vectors.dtype == torch.float64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdCrgkDXfqGg"
      },
      "source": [
        "### Calculate vector similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULUM2lhop6FT"
      },
      "source": [
        "To measure how similar are vectors we use cosine similariry. It`s formula is represented below.<br>\n",
        "When we need to get deeper and faster insight into data, without clustering we can just calculate cosine similarity between vectors and get first understanding how common is concrete vector to other vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUDuiFORFx5d"
      },
      "source": [
        "#### Task 2\n",
        "Write on pure numpy function that measure vector distance between two vectors. Each input vector is a numpy array. Return a single float number that measure similarity between vectors.\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktAtfesGrSDb"
      },
      "source": [
        "##### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iViNG3nNrUkS"
      },
      "source": [
        "# 1. In numerator we get scalar multiplication of two vectors. It sums pairwise multiplication of each component in vectors\n",
        "# 2. In denominator we get multiplication of length of each vector. We calculate it as L2-norm.\n",
        "\n",
        "# You can use numpy functions below to calculate similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-5PRIwKGEBt"
      },
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IzvtuwzrU-z"
      },
      "source": [
        "##### continue work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLKSps22GRvn"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def cos_similarity(a: np.array, b: np.array) -> float:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQpBP0jWGVW9"
      },
      "source": [
        "cos_sim_1 = cos_similarity(normed_vectors[0], normed_vectors[1])\n",
        "cos_sim_2 = cos_similarity(normed_vectors[1], normed_vectors[2])\n",
        "\n",
        "assert cos_sim_1 < cos_sim_2\n",
        "assert cos_sim_1 > 0.96\n",
        "assert cos_sim_2 > 0.98"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8RQdM1gf07U"
      },
      "source": [
        "### Training vector representation from our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9hU90Kas0L5"
      },
      "source": [
        "If we have anough data or you are not familier with domain of your data you can train language model from just your data. After traing you can explore your vectors with some clusterning technics or just visualize vectors in 2d or 3d dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUL27zXstk_t"
      },
      "source": [
        "On lecture we discussed word2vec model. More complex and functional version of word2vec is Fasttext, that handle main problem of word2vec - Out-Of-Vocabulary words. Implementation of this model and some basic functionality with it you can find in python library fasttext. It is disigned by Facebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoJFdx8QHb8J"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNhGth4MuNiL"
      },
      "source": [
        "Download data of russian comments form Odnoklassniki social network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k7tcvztISpy"
      },
      "source": [
        "# !wget https://raw.githubusercontent.com/snv-ds/NLP_course/master/week2/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELfxbU7Oux2T"
      },
      "source": [
        "Importing fasttext library to use FastText model implemented there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M0UUTTPOvG_"
      },
      "source": [
        "import fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAEwvVIVu5p6"
      },
      "source": [
        "Our data is multilabel classification task. Classes represented in data you can see below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9BHjqb1KIfV"
      },
      "source": [
        "parse_labels = ['__label__NORMAL','__label__INSULT','__label__THREAT','__label__OBSCENITY']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4a2ZNpLvHNd"
      },
      "source": [
        "Here we will load data, prepocess each row to get features as OHE vectors and translate target features on russian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2C_favhIyib"
      },
      "source": [
        "train = pd.read_csv('data/train', sep='\\t', names=['id','target','temp1','temp2','comment'], index_col=0)\n",
        "\n",
        "mask = train['comment'].isin(parse_labels)  # to cope only with correct rows in data\n",
        "train.loc[mask,'target'] = train[mask]['target'] + ',' + train[mask]['comment']\n",
        "train.loc[mask,'comment'] = np.nan\n",
        "\n",
        "for t in ['temp1','temp2']: # if comment have several labels of classes\n",
        "    mask = train[t].isin(parse_labels)\n",
        "    train.loc[mask,'target'] = train[mask]['target'] + ',' + train[mask][t]\n",
        "    train.loc[mask,t] = np.nan\n",
        "    train.loc[~train[t].isna(),'comment'] = train[~train[t].isna()][t]\n",
        "\n",
        "train[['оскорбление','другое','непростойность','угроза']] = train['target'].str.get_dummies(sep=',')\n",
        "\n",
        "train = train[['другое','оскорбление','непростойность','угроза', 'comment']]\n",
        "train.sample(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2a7QbZSLuKa"
      },
      "source": [
        "#### Task 3\n",
        "Initialize variables for texts and targets from train data. Next time we will process this variables in our language model.<br> This action is very frequent and we want to automate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjhVJ-HQwOyx"
      },
      "source": [
        "Each variable will contain np.array values from our train pandas.DataFrame. <br>Numpy is very fast to process, thats why we try to use it more often."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbuZa-gEL4fd"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "texts = None\n",
        "targets = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRUj_KhbLic4"
      },
      "source": [
        "assert texts.shape[0] == 148775\n",
        "assert targets.shape[1] == 4\n",
        "assert texts.shape[0] == targets.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCWFfKvVNhTt"
      },
      "source": [
        "#### Task 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YBmSSjowyco"
      },
      "source": [
        "To load data in FastText model we need to correctly represent it. FastText works only with corpus, where every document is splited by `\\n` symbol. When we train model on our data we can then represent each word in it as vector and continue exploring data from mathematical point of view."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGD2ncLbwxIz"
      },
      "source": [
        "Create file `messages_unsupervised.txt`, that will contain all messages separeted by the `\\n` symbol. This file we will futher use to train FastText model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYOvFvopMlee"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WfYJUn4MlSE"
      },
      "source": [
        "import os\n",
        "\n",
        "assert 'messages_unsupervised.txt' in os.listdir('./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkEBGqcAzhpb"
      },
      "source": [
        "Start training vector representation of words in our data is very simple. We can just pass generated file in method `train_unsupervised` of fasttext library and set some parameters. Fasttext model use ngams of symbols in words to handle OOV problem. By setting params `minn` and `maxn` we set number of ngrams we will generate. Param `dim` set output dimension of our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSwekuz7Oipe"
      },
      "source": [
        "%%time\n",
        "ft_vectors = fasttext.train_unsupervised('messages_unsupervised.txt', minn=3,maxn=5, dim=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmL4CgGB0bMk"
      },
      "source": [
        "Now we can see most similar vectors in our dataset to given word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xaypsl-Ozhq"
      },
      "source": [
        "ft_vectors.get_nearest_neighbors('дебил')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7AC35CMP3Q4"
      },
      "source": [
        "Let`s visualize our vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKr-zH1z0kvc"
      },
      "source": [
        "For visualization we first need to reduce dimension of our data to 2d or 3d, because only this we can illustrate.<br>\n",
        "One of appropriate methods, that we can use to reduce dimensions is TSNE. It is implemented in MulticoreTSNE python library. <br>\n",
        "Another methods that can reduce dimension is:\n",
        "- UMAP (implemeted in umap library)\n",
        "- SVD (implemented in sklearn)\n",
        "- PCA (implemented in sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxg9eHKeP24j"
      },
      "source": [
        "!pip install MulticoreTSNE > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urq1Qcc5QAnm"
      },
      "source": [
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from MulticoreTSNE import MulticoreTSNE as TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lt3hly11VKo"
      },
      "source": [
        "We have so much words in our dataset. Lets take only 3 000 most frequent one, to visualize only them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIIu3_66QAbb"
      },
      "source": [
        "print(len(ft_vectors.words))\n",
        "top3k = ft_vectors.words[:3000]\n",
        "top3k[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBZngduqQK-z"
      },
      "source": [
        "#### Task 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAnOViB11kpN"
      },
      "source": [
        "initialize `top3k_vectors` variable with all vectors, that are in top 3000 for frequency.<br> \n",
        "Result variable will be list of vectors. Each vector present concrete word representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N6uodJBQY65"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "top3k_vectors = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ2OK989Qegr"
      },
      "source": [
        "%%time\n",
        "tsne_emb = TSNE(n_components=2,\n",
        "                n_iter=2000,\n",
        "                n_jobs=-1,\n",
        "                random_state=42).fit_transform(np.array(top3k_vectors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH66cbquQeaI"
      },
      "source": [
        "from bokeh.models import ColumnDataSource, LabelSet\n",
        "from bokeh.plotting import figure, show, output_file\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "output_notebook()\n",
        "\n",
        "p = figure(tools=\"pan,wheel_zoom,reset,save\", title=\"TSNE representation of FastText vectors (top 3k words)\")\n",
        "\n",
        "source = ColumnDataSource(data=dict(x1=tsne_emb[:,0], x2=tsne_emb[:,1], names=top3k))\n",
        "\n",
        "p.scatter(x=\"x1\", y=\"x2\", source=source, size=5)\n",
        "\n",
        "words = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", source=source,\n",
        "                y_offset=6, text_font_size=\"6pt\", \n",
        "                 text_color=\"#555555\", text_align=\"center\")\n",
        "\n",
        "p.add_layout(words)\n",
        "\n",
        "show(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGMIgssJREoy"
      },
      "source": [
        "Now try to preprocess texts and train fasttext one more time!<br>\n",
        "write function to preprocess texts. You can do whatever you want:\n",
        " - delete stop words\n",
        " - delete punctuation\n",
        " - lematize words\n",
        " - stemming\n",
        " - lower texts\n",
        "\n",
        " After that you can explore texts and their representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8vIXQnXR6ko"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ozCC-s_d9M"
      },
      "source": [
        "In this task we will solve task of classification sentiment of text. For this purpose install python library datasets which contains IMDB dataset. This is dataset for binary classification. <br>\n",
        "Our main goals are:\n",
        "- solve real task;\n",
        "- try to write custom classes for processing data;\n",
        "- use pretrained vector representaition (aka transfer learning);\n",
        "- try new arhitecture for classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkPwemZ2QeTL"
      },
      "source": [
        "!pip install datasets > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TspVDW9TSiXP"
      },
      "source": [
        "import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRnJBY1ZCC4R"
      },
      "source": [
        "Loading IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Uulr7deSgto"
      },
      "source": [
        "text_dataset = datasets.load_dataset(\"imdb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqQT9KKxS3ui"
      },
      "source": [
        "pd.DataFrame(text_dataset['train'], columns=['label', 'text'])['label'].hist();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eW-5HDb6167"
      },
      "source": [
        "df = pd.DataFrame(text_dataset['train'], columns=['label', 'text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3DEZgV4D1rr"
      },
      "source": [
        "#### Task 6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agwtsDBECkyE"
      },
      "source": [
        "This dataset is rather large, we don`t need so much data. Let's trim our dataset and take only part of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1hW4JTMCJvW"
      },
      "source": [
        "Initialize new df as half of randomly chosen rows of df.<br>\n",
        "You can use different built-in pandas methods, such as sample, loc, iloc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEoLR5fxECT2"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yza7pT9U80H-"
      },
      "source": [
        "df['label'].hist();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qkPbX75CyNz"
      },
      "source": [
        "Before we start working with text, we need to learn how to initialize some layer of neural network with pretrained or predefined vectors.<br> We will use this mechanism if work with pretrained words representaion or pretrained layers of other network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZUP7KqKS5nK"
      },
      "source": [
        "# initialize some vectors\n",
        "some_vectors = torch.rand((10,50), dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZo4j0iZVGOC"
      },
      "source": [
        "# some random linear layer of neural network\n",
        "custom_layer = nn.Embedding(10, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTwkk5hgVGFH"
      },
      "source": [
        "# initialize weights of defined layer with early given vectors\n",
        "custom_layer.weight = nn.Parameter(some_vectors.clone().detach().requires_grad_(True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qebgNTs_D--B"
      },
      "source": [
        "#### Downloading pretrained vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQSN4KKVEEFM"
      },
      "source": [
        "In NLP we have some libraries, that contains pretrained word representaions trained on large corpuses of texts. Some times they contains word representations for several languages (also russian).<br>\n",
        "One library, that we will use `Gensim`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt44kXP2S5gG"
      },
      "source": [
        "import gensim.downloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXXqT0C7El4M"
      },
      "source": [
        "We can see what models represented in whis library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THX-SwDBTfOt"
      },
      "source": [
        "list(gensim.downloader.info()['models'].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8S1f2nrEz5k"
      },
      "source": [
        "Gensim contains vector representaion not only for word2vec, but for `Glove` and `Fasttext`. All of them give us realization of distributional semantics converted in different, but rather similar approaches.<br>\n",
        "We will use `glove` vectors just for their light weights and appropriate representation. You can try some other models by yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIZwT-ROS5W4"
      },
      "source": [
        "glove_vectors = gensim.downloader.load('glove-twitter-100')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwCbnL3iTwsQ"
      },
      "source": [
        "glove_vectors.vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAeA6mZNZtfQ"
      },
      "source": [
        "list(glove_vectors.vocab.keys())[250:300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4HKyi3aVu2z"
      },
      "source": [
        "#### Task 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FePD8iQF5wp"
      },
      "source": [
        "Initialize loaded weights from gensim to some embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4JHhXK4GusL"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omhfJYfCV33e"
      },
      "source": [
        "assert embedding_matrix.shape == (1193514, 100)\n",
        "assert embedding_matrix[23].sum() < -4.45\n",
        "assert cos_similarity(embedding_matrix[42], embedding_matrix[23]) < 0.57"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sLJ7ctqgctS"
      },
      "source": [
        "After get loaded pretrained weights we can get back to explore data. We will use simple nltk WordPuckt tokenizer to split  punctuation and words from each other. Also you can try here TwitterTokenizer or some other methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqwXB5bya5Vl"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9iLspTMGeTE"
      },
      "source": [
        "Look at the lengths of each sample of data and collect in list `l`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORU3mXBVZ8zm"
      },
      "source": [
        "l = float('-inf')\n",
        "l = list()\n",
        "for text in tqdm(df['text']):\n",
        "    l.append(len(tokenizer.tokenize(text.lower())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpQtXnOXZ8tW"
      },
      "source": [
        "print(pd.Series(l).describe())\n",
        "pd.Series(l).plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEPRWkqtGtY4"
      },
      "source": [
        "If you see outliers of length you can preprocess your data and delete or split such long examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePB81trGcH-n"
      },
      "source": [
        "it can be seen that 90% of the texts do not exceed 1500 tokens. Let's take the quantile 0.95 with a margin and initialize the variable max_len with this value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS10uwo2EqSu"
      },
      "source": [
        "#### Task 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYlAZWg5HjcP"
      },
      "source": [
        "Just for simplicity we will delete samples, that have legth over some border value.<br>\n",
        "Take the quantile 0.95 of `l` with a margin and initialize the variable max_len with this value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkCpvUhHcCZ4"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "max_len = None\n",
        "max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOQhQdv-H_uc"
      },
      "source": [
        "Create vocab of each token in our dataset.<br>\n",
        "Some approaches to fix their order:\n",
        "- we can count frequencies of each token, enumerate their order and fix it;\n",
        "- use countvectorizer form sklearn;\n",
        "- lexicographically sort all tokens in vocab and fix order.<br>\n",
        "\n",
        "You can come up with your own method - it`s all up to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0zLOpdFh4qJ"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HBCYTicJn3Y"
      },
      "source": [
        "We will count all tokens and fix their ranks. <br>\n",
        "All unknown tokens, that are not present in vocab we will translate into special `<UNK>` token, thats why we add it in vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNtTEj9X2IxF"
      },
      "source": [
        "words = list()\n",
        "for text in tqdm(df['text']):\n",
        "    words.extend(tokenizer.tokenize(text.lower()))\n",
        "index2word = dict(enumerate((word[0] for word in Counter(words).most_common()), start=1))\n",
        "index2word[0] = '<UNK>'\n",
        "word_index = {word: ind for ind, word in index2word.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7KzEn9KjX4y"
      },
      "source": [
        "len(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWvO8k4KJ0r"
      },
      "source": [
        "After fixing order of words we can create metrix of vectors.<br>\n",
        "In each row their will be vector for corresponding word. Index of word is equal to index in our vocabulary.<br>\n",
        "We will take pretrained vectors from uploaded glove vectors representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5zq9Qyvked5"
      },
      "source": [
        "def load_w2v(words_dict, w2v, embed_size=100, max_features=100_000):\n",
        "  emb_matrix = np.zeros((min(max_features, len(words_dict)), embed_size))\n",
        "  for word, ind in words_dict.items():\n",
        "      if ind >= max_features:\n",
        "        break\n",
        "      try:\n",
        "        embedding_vector = w2v.get_vector(word)\n",
        "      except KeyError:\n",
        "        embedding_vector = None\n",
        "      if embedding_vector is not None:\n",
        "        emb_matrix[ind] = embedding_vector\n",
        "  return normalize_vectors(emb_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUH7heH2meFb"
      },
      "source": [
        "embedding_matrix = load_w2v(word_index, glove_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu867nGBMFCI"
      },
      "source": [
        "Some times we need to write custom classes for dataset to pass our data into model.<br>\n",
        "We write our class, and each document will be eaqual size, for simple processing by model. Size of each train sample tensor we choose as `max_len` - all samples that are longer will be trimmed by our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8RmGeyaby64"
      },
      "source": [
        "class CustomDataset:\n",
        "  def __init__(self, data, targets, tokenize, word2index, max_len=max_len):\n",
        "    self.data = data\n",
        "    self.targets = targets\n",
        "    self.tokenize = tokenize\n",
        "    self.word2index = word2index\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.data[idx]\n",
        "    label = self.targets[idx]\n",
        "\n",
        "    input_ids = np.zeros((int(max_len),))\n",
        "    row = self.tokenize(text.lower())\n",
        "    for ind, word in enumerate(row):\n",
        "      if ind < max_len - 1:\n",
        "        input_ids[ind] = self.word2index.get(word, 0)\n",
        "    \n",
        "    return {\n",
        "        'text': torch.tensor(input_ids, dtype=torch.long),\n",
        "        'label': torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV1phAtgDOS_"
      },
      "source": [
        "##### Task 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fksDhf6O1ax3"
      },
      "source": [
        "Create `train_dataset` variable as instance of class CustomDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNV7HuCGF49T"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JXOa-yui066"
      },
      "source": [
        "train_dataset[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30QTtIEiOLDq"
      },
      "source": [
        "For this task we try to train Convolutional Neural Network. You can use more simplier architecture for this task.<br>\n",
        "In text classification tasks, if you deal with data short(less than 50 tokens) and middle length(from 50 to 350) you can succesfully use CNN. Other approach we will explore in next seminars."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3zAsOSmqvkg"
      },
      "source": [
        "class ConvNeuralNet(nn.Module):\n",
        "    def __init__(self, embedding_matrix, embed_size,\n",
        "                 do_rate1=0.5, n_classes=2):\n",
        "        super(ConvNeuralNet, self).__init__()\n",
        "\n",
        "        self.vocab_size = embedding_matrix.shape[0]\n",
        "        self.embedding_size = embed_size\n",
        "        self.kernel_num = 128\n",
        "        self.kernels_sizes = [2, 4, 8]\n",
        "        \n",
        "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))  # pretrained initialization\n",
        "        self.embedding.weight.requires_grad = False # freeze weights\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, self.kernel_num, (K, embed_size)) for K in self.kernels_sizes])\n",
        "        self.dropout = nn.Dropout2d(do_rate1)\n",
        "        self.fc1 = nn.Linear(len(self.kernels_sizes) * self.kernel_num, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)\n",
        "        \n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        \n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        \n",
        "        x = torch.cat(x, 1)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        logit = self.fc1(x)\n",
        "        return logit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmS7HqpqE5mQ"
      },
      "source": [
        "##### Task 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uviyh0wC1kco"
      },
      "source": [
        "Fill the code with calculation of making prediction, calculating loss, backpropogation, optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFt83Qavsop-"
      },
      "source": [
        "we initialize device in such way for more flexibility (works on cpu and cuda)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTaemKcjhjq1"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else  torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icq90ojnsIA6"
      },
      "source": [
        "For text classification task we use cross_entropy loss function as optimization functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHXP06ORFcru"
      },
      "source": [
        "# hyperparameters\n",
        "EPOCHS = 7\n",
        "LR = 3e-4 \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "model = ConvNeuralNet(embedding_matrix, 100).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4) # for first iter Adam is one of the best optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djuVYZAHFONb"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def train(dataloader, optimizer=optimizer, criterion=criterion):\n",
        "    model.train()\n",
        "    total_acc, total_count, total_loss = 0, 0, 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, batch in enumerate(tqdm(dataloader)):\n",
        "        text = batch['text'].to(device)\n",
        "        label = batch['label'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # here you need to get predictions of model\n",
        "        # FILL HERE\n",
        "        # after that you calculate loss from predictions and true values\n",
        "        # loss = FILL HERE\n",
        "        # next you run backpropogation through your model\n",
        "        # FILL HERE\n",
        "        total_loss += loss\n",
        "        # and finally make an optimizer step\n",
        "        # FILL HERE\n",
        "        predited_label = predited_label.detach().cpu().numpy()\n",
        "        label = label.to('cpu').numpy()\n",
        "        total_acc += accuracy(predited_label, label)\n",
        "        total_count += len(label)\n",
        "    elapsed = time.time() - start_time\n",
        "    print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}| loss {:8.3f}'.format(epoch, idx,\n",
        "                                                            len(dataloader),\n",
        "                                                            total_acc/total_count,\n",
        "                                                            total_loss/total_count\n",
        "                                                            ))\n",
        "    total_acc, total_count = 0, 0\n",
        "    start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIDke2EcgR5_"
      },
      "source": [
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count, total_loss = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(dataloader):\n",
        "            text = batch['text'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "            predited_label = model(text)\n",
        "            total_loss += criterion(predited_label, label)\n",
        "            predited_label = predited_label.detach().cpu().numpy()\n",
        "            label = label.to('cpu').numpy()\n",
        "            total_acc += accuracy(predited_label, label)\n",
        "            total_count += len(label)\n",
        "    print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "    '| accuracy {:8.3f}| loss {:8.3f}'.format(epoch, idx,\n",
        "                                              len(dataloader),\n",
        "                                              total_acc/total_count,\n",
        "                                              total_loss/total_count)\n",
        "    )\n",
        "    return total_acc/total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9rvUlMXheoe"
      },
      "source": [
        "def accuracy(probs, targets):\n",
        "    outputs = np.argmax(probs, axis=1)\n",
        "    return np.sum(outputs == targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35utxjREtp3O"
      },
      "source": [
        "After writing train and evaluate functions we got ready to start training process. We will get 20% of our train data for validation. On validation metrics we control process of training and prevent overfitting on train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Af924ehsSh"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "import time\n",
        "\n",
        "total_accu = None\n",
        "\n",
        "num_train = int(len(train_dataset) * 0.8)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True) \n",
        "valid_dataloader = torch.utils.data.DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    # if epoch == 4: # lets unfreeze weights of model after some epochs\n",
        "    #   model.embedding.weight.requires_grad = True\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUHOdgKluXlP"
      },
      "source": [
        "After finish training process, we can use our model to classify real samples of comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w1YOZSyfjdN"
      },
      "source": [
        "positive_comment = 'This movie is awesome!'\n",
        "negative_comment = 'This movie is very very bad!'\n",
        "neutral_comment = 'This is so-so'\n",
        "\n",
        "_dataset = CustomDataset([positive_comment, negative_comment, neutral_comment], [0] * 3, tokenizer.tokenize, word_index)\n",
        "_dataloader = torch.utils.data.DataLoader(_dataset, batch_size=32)\n",
        "\n",
        "_x = next(iter(_dataloader))['text'].to(device)\n",
        "probs = torch.softmax(model(_x), 1).detach().cpu().numpy()\n",
        "\n",
        "print(np.round(probs, 3))\n",
        "\n",
        "print(f'Probability of \"{positive_comment}\" to be positive -- {probs[0][1]}')\n",
        "print(f'Probability of \"{negative_comment}\" to be positive -- {probs[1][0]}')\n",
        "print(f'Probability of \"{neutral_comment}\" to be positive -- {probs[2][0]}')\n",
        "\n",
        "del _dataset, _dataloader "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
