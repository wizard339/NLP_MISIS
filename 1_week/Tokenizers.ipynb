{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenizers",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI_L2TBSZadk",
        "outputId": "8367ad77-80e6-4b8f-ee0c-f2f49905ddcf"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install tokenizers==0.10.0rc1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: tokenizers==0.10.0rc1 in /usr/local/lib/python3.6/dist-packages (0.10.0rc1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s09CQfFyURY8"
      },
      "source": [
        "# ü§ó Tokenizers\n",
        "\n",
        "–î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º –Ω–∞–º –Ω—É–∂–Ω–æ —É–º–µ—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å –µ–≥–æ –≤ –ø–æ–Ω—è—Ç–Ω–æ–º –∫–æ–ø–º—å—é—Ç–µ—Ä—É –≤–∏–¥–µ –∏ –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏ –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ - —ç—Ç–æ —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã (–∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–æ–≤–∞–º–∏, –∞ –º–æ–≥—É—Ç –∏ –Ω–µ –±—ã—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏) –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä –æ—Ç 0 –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤–∞—à–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º—ã —Å–º–æ–∂–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫—É —Ç–∏–ø–∞ \"—ç—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞\" –≤ –≤–µ–∫—Ç–æ—Ä –≤–∏–¥–∞ `[47, 392, 38]`. –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è - –∑–∞–º–µ–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã - —á–∞—Å—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è**, –∞ –æ–±—Ä–∞—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è - **–¥–µ–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è**.\n",
        "\n",
        "–ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –º–æ–¥—É–ª—å `sklearn.feature_extraction.text`, –Ω–∞–ø—Ä–∏–º–µ—Ä –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é `TfidfVectorizer`, sklearn –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ —ç—Ç–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∑–∞ –≤–∞—Å –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º, –Ω–æ –µ—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, –∂–∏–∑–Ω—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≥–æ—Ä–∞–∑–¥–æ —Å–ª–æ–∂–Ω–µ–µ –∏ –¥–æ –Ω–µ–¥–∞–≤–Ω–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—á—Ç–∏ –≤—Å–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Å–∞–º–æ–ø–∏—Å–Ω—ã–µ —à—Ç—É–∫–∏, —á—Ç–æ–±—ã (–¥–µ)–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã. –≠—Ç–æ —Å–æ–∑–¥–∞–≤–∞–ª–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–µ—É–¥–æ–±—Å—Ç —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ç–µ–º, —á—Ç–æ –≤–æ-–ø–µ—Ä–≤—ã—Ö –≤–∞–º –ø—Ä–∏—Ö–æ–¥–∏–ª–æ—Å—å –ø–∏—Å–∞—Ç—å –±–æ–ª—å—à–µ –∫–æ–¥–∞ (–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–æ–ª—å—à–µ –æ—à–∏–±–æ–∫), –≤–æ-–≤—Ç–æ—Ä—ã—Ö —Ä—è–¥–æ–º —Å –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏–µ–π –ª–µ–∂–∏—Ç –æ—á–µ–Ω—å –º–Ω–æ–≥–æ –º–µ–ª–∫–∏—Ö –ø–æ–¥–∑–∞–¥–∞—á –≤–∫–ª—é—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ UNK, —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞—à–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∏ —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è. \n",
        "\n",
        "Huggingface Tokenizers –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤—Å—ë –±–æ–ª–µ–µ –∏ –±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –≤ NLP-—Å–æ–æ–±—â–µ—Å—Ç–≤–µ. –≠—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –±—ã—Å—Ç—Ä–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ - –æ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ —Å–ª–æ–≤–∞ –ø–æ –ø—Ä–æ–±–µ–ª—É –¥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –±–∏—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è unicode-—Å—Ç—Ä–æ–∫ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —á–∞—Å—Ç–æ—Ç—ã –±–∏—Ç–æ–≤–æ–π n-gram. –¢–∞–∫–∂–µ –æ–Ω–∞ —É–º–µ–µ—Ç –¥–µ–ª–∞—Ç—å –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—é –∏ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ —Ä–∞–±–æ—Ç–∞–µ—Ç. –í–º–µ—Å—Ç–æ —Å–∞–º–æ–ø–∏—Å–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—ë –µ—â—ë —á–∞—Å—Ç–æ –ø—Ä–µ–ø–æ–¥–∞—é—Ç—Å—è –≤ –∫—É—Ä—Å–∞—Ö –ø–æ NLP, –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ü§ó Tokenizers –≤ —Ç–µ—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ –∫—É—Ä—Å–∞ –∏ –¥–æ–≤–æ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–æ –≤—ã –Ω–∞—É—á–∏—Ç–µ—Å—å –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–∞–∂–µ –Ω–µ —Å–∞–º—ã–º–∏ –æ—á–µ–≤–∏–¥–Ω—ã–º–∏ –µ—ë –º–µ—Ç–æ–¥–∞–º–∏. –ê —Å–µ–π—á–∞—Å –º—ã –Ω–∞—á–Ω—ë–º —Å —Å–∞–º—ã—Ö –æ—Å–Ω–æ–≤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "DLYms6MBUPYm",
        "outputId": "5df803b9-1e0b-4515-8212-99d3e244e48c"
      },
      "source": [
        "import datasets\n",
        "import tokenizers\n",
        "tokenizers.__version__  # should be above or equal 0.10.0rc1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.10.0rc1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww0-lHJwaSMr"
      },
      "source": [
        "–ó–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –µ–≥–æ –Ω–∞ —Ç–æ–∫–µ–Ω—ã –∏ –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ ü§ó Tokenizers –æ—Ç–≤–µ—á–∞–µ—Ç –æ–±—ä–µ–∫—Ç `Tokenizer`. –°–∞–º—ã–π –Ω–∞–≥–ª—è–¥–Ω—ã–π —Å–ø–æ—Å–æ–± –µ–≥–æ —Å–æ–∑–¥–∞—Ç—å - —ç—Ç–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –µ–º—É —Å–ª–æ–≤–∞—Ä—å, –º–∞–ø—è—â–∏–π —Å–ª–æ–≤–∞ –Ω–∞ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã. –î–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —ç—Ç–æ –¥–µ–ª–∞–µ—Ç.\n",
        "\n",
        "**NOTE:** –°–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ - —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ —Å–ª–æ–≤–∞ —Å –ø–æ–º–æ—â—å—é regex - —Å—Ç–æ–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–±–æ–∫—É –≤ Tokenizers –∏ –¥–∞–∂–µ –Ω–µ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, —Ç–∫ –∑–∞—á–∞—Å—Ç—É—é –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –ø–µ—Ä–≤—ã–π —à–∞–≥ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –ü–æ—ç—Ç–æ–º—É –æ–Ω–∞ –∑–∞–¥–∞—ë—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—ä–µ–∫—Ç–∞ –≤–∏–¥–∞ `pre_tokenizer`.\n",
        "\n",
        "–î–ª—è —ç—Ç–æ–≥–æ –º—ã –¥–µ–ª–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤ –Ω–∞—á–∞–ª–µ —Å–æ–∑–¥–∞—ë—Ç –æ–±—ä–µ–∫—Ç `pre_tokenizer` –∏ –ø—Ä–µ–ø–æ—Ü–µ—Å—Å–∏—Ç –Ω–∞—à–∏ —Ç–µ–∫—Å—Ç—ã. –ü–æ—Å–ª–µ —á–µ–≥–æ –º—ã –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –Ω–∞—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –≤—ã–±–∏—Ä–∞–µ—Ç top-N —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö, —ç—Ç–æ —É–¥–æ–±–Ω–æ –¥–µ–ª–∞—Ç—å —Å –ø–æ–º–æ—â—å—é –æ–±—ä–µ–∫—Ç–∞ `Counter` –∏–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –≤ Python –º–æ–¥—É–ª—è collections. –í –∏–≥—Ä—É—à–µ—á–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ –≤ —Å–ª–µ—â—É—é—â–µ–π —è—á–µ–π–∫–µ, —Å–æ—Å—Ç–æ—è—â–∏–º —Ç–æ–ª—å–∫–æ –ª–∏—à—å –∏–∑ –¥–≤—É—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —ç—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è –Ω–µ–≤–∞–∂–Ω–∞, –Ω–æ –∫–æ–≥–¥–∞ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ —Å –í–∏–∫–∏–ø–µ–¥–∏—é, –≤–∞—à —Å–ª–æ–≤–∞—Ä—å –±—ã—Å—Ç—Ä–æ —Å—Ç–∞–Ω–µ—Ç –Ω–µ–æ–±—ä—ë–º–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ –µ–≥–æ –Ω—É–∂–Ω–æ —É–º–µ—Ç—å –æ–≥—Ä–Ω–∏—á–∏–≤–∞—Ç—å.\n",
        "\n",
        "–¢—Ä–µ—Ç–∏–π —à–∞–≥ - —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–∏–Ω–≥–∞ —Å–ª–æ–≤ –Ω–∞ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã. –û–Ω –¥–æ–≤–æ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –∏ –µ–≥–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ—á–∫—É —Å –ø–æ–º–æ—â—å—é dictionary comprehension. –ò –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º—ã –≥–æ—Ç–æ–≤—ã —Å–æ–∑–¥–∞—Ç—å –æ–±—ä–µ–∫—Ç `Tokenizer` –∏ –ø—Ä–∏—Å–≤–æ–∏—Ç—å –µ–º—É –ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPZ9Exn3ZeKa"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "texts = ['a list of sentences from my dataset', 'this is a text with known words']\n",
        "\n",
        "\n",
        "def make_whitespace_tokenizer(texts, max_vocab_size=10_000, unk_token='UNK'):\n",
        "    pre_tokenizer = Whitespace()\n",
        "    tokenized_texts = [[w for w, _ in pre_tokenizer.pre_tokenize_str(t)] for t in texts]\n",
        "\n",
        "    c = Counter()\n",
        "    for text in tokenized_texts:\n",
        "        c.update(text)\n",
        "\n",
        "    token2id = {word: i + 1 for i, (word, count) in enumerate(c.most_common(max_vocab_size))}\n",
        "\n",
        "    # usually, UNK is assigned index 0 or 1\n",
        "    token2id[unk_token] = 0\n",
        "\n",
        "    tokenizer = tokenizers.Tokenizer(WordLevel(token2id, unk_token))\n",
        "    tokenizer.pre_tokenizer = pre_tokenizer\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "tokenizer = make_whitespace_tokenizer(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpbn6P6Qd2Jq"
      },
      "source": [
        "# Encoding and decoding text\n",
        "\n",
        "–î–ª—è –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ `.encode()`, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∏–π –æ–±—ä–µ–∫—Ç –∫–ª–∞—Å—Å–∞ `Encoding`. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –≤ —Å–µ–±–µ –º–Ω–æ–≥–æ –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–æ —Å–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ –∏–∑ –Ω–µ–≥–æ - `.ids` –¥–∞—é—â–∏–π –Ω–∞–º –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –≥–¥–µ —Å–ª–æ–≤–∞ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rF_Ouq4bg8k",
        "outputId": "0fd8660c-dc39-43ef-b08e-543b7d01a80e"
      },
      "source": [
        "_text = 'this is a text with unknown_word'\n",
        "e = tokenizer.encode(_text)\n",
        "e"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM1wIMLIiFD_",
        "outputId": "1df417dd-f16f-4b75-d0bb-1f6cad0c1920"
      },
      "source": [
        "e.ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 9, 1, 10, 11, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEb3MVLueR8V"
      },
      "source": [
        "–î–µ–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–ª–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º `.decode()` (–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –Ω–∞ –≤—Ö–æ–¥ –∫ –Ω–µ–º—É –ø—Ä–∏—Ö–æ–¥—è—Ç `.ids`, –∞ –Ω–µ –æ–±—ä–µ–∫—Ç `Encoding`). –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è –æ—á–µ–Ω—å –≤–∞–∂–Ω–∞ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –ø–æ –º–Ω–æ–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º –∏ –æ–¥–Ω–∞ –∏–∑ –Ω–∏—Ö - –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Ç–µ–∫—Å—Ç —Ç–∞–∫, –∫–∞–∫ –µ–≥–æ –≤–∏–¥–∏—Ç –≤–∞—à–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å - —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–∏ —Å–ª–æ–∞–≤–º–∏ –∑–∞–º–µ–Ω–µ–Ω–Ω—ã–º–∏ –Ω–∞ —Ç–æ–∫–µ–Ω UNK –∏, –≤–æ–∑–º–æ–∂–Ω–æ, —Å –≤–∞–º–∏—à–∏ –æ—à–∏–±–∫–∞–º–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞. –≠—Ç–∏ –æ—à–∏–±–∫–∏ –æ—á–µ–Ω—å —á–∞—Å—Ç—ã –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–µ–Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–∏–ª—å–Ω–æ –ø–æ–º–æ–≥–∞–µ—Ç –≤ –¥–µ–±–∞–≥–µ.\n",
        "\n",
        "–ö—Ä–æ–º–µ —ç—Ç–æ–≥–æ –Ω–∞–º –Ω–µ –æ–±–æ–π—Ç–∏—Å—å –±–µ–∑ —ç—Ç–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞ –≤—ã—Ö–æ–¥–µ –∏–∑ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –º—ã –æ–∂–∏–¥–∞–µ–º –Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∏—Å–µ–ª, –∞ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–π –Ω–∞ –¥—Ä—É–≥–æ–π —è–∑—ã–∫ —Ç–µ–∫—Å—Ç."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "vI3sByQyeNPs",
        "outputId": "1fa36cfb-7ab2-48cb-c4cc-14046b80f3c1"
      },
      "source": [
        "tokenizer.decode(e.ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is a text with UNK'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQAq6d9dfqrr"
      },
      "source": [
        "–í—ã –º–æ–∂–µ—Ç–µ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—Ç—Ç—Ä–∏–±—É—Ç `.tokens` —É –æ–±—ä–µ–∫—Ç–∞ `Encoding` –∏ –æ–Ω –≤—ã–¥–∞—Å—Ç –≤–∞–º —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —ç—Ç–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –≤ –Ω—ë–º —Ö—Ä–∞–Ω—è—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Ç–µ–∫—Å—Ç–∞, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –µ–≥–æ –Ω–µ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –¥–µ–±–∞–≥–∞."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS1sTaocfUQj",
        "outputId": "804edf6f-bc32-4473-e664-63ead4d571e4"
      },
      "source": [
        "e.tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'a', 'text', 'with', 'unknown_word']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2lOuyQGgHde"
      },
      "source": [
        "–ï—â—ë –æ–¥–Ω–∞ –≤–∞–∂–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è `Tokenizer` - —ç—Ç–æ –¥–∞–≤–∞—Ç—å –≤–∞–º id –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç - –≤—ã–¥–∞–≤–∞—Ç—å —Å–ª–æ–≤–æ –ø–æ –µ–≥–æ id. –≠—Ç–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–µ–ª–∞—é—Ç—Å—è –º–µ—Ç–æ–¥–∞–º–∏ `token_to_id` –∏ `id_to_token`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC-_wQZ-frw2",
        "outputId": "e849eeaf-c535-42a5-e002-1c59f9eb30a3"
      },
      "source": [
        "tokenizer.token_to_id('this')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "xFKByRI7gO3u",
        "outputId": "4e4d4f3c-4d1a-4282-c26d-81d8dd89a994"
      },
      "source": [
        "tokenizer.id_to_token(8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMhM0M-agZ1P"
      },
      "source": [
        "# Saving and loading the tokenizer\n",
        "\n",
        "–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –∑–∞–¥–µ–ø–ª–æ–∏—Ç—å –≤–∞—à—É –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –≤–∞–º –Ω—É–∂–Ω—ã –¥–≤–µ –≤–µ—â–∏: –≤–∞—à–∞ –º–æ–¥–µ–ª—å –∏ –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞. `Tokenizer` –ø–æ —Å—É—Ç–∏ –∏ —è–≤–ª—è–µ—Ç—Å—è —ç—Ç–∏–º –º–µ—Ç–æ–¥–æ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞, —Ç–∫ –æ–Ω –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤: –Ω–æ—Ä–º–∞–ª–∏–∑—Ü–∏—è –∏ –ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —Ä–∞–∑–±–∏–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã –∏ –Ω—É–º–µ—Ä–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ `Tokenizer` —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUIQQsgfgVam"
      },
      "source": [
        "tokenizer.save('tokenizer.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj-V1eKfhVeG"
      },
      "source": [
        "–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç —ç—Ç–æ—Ç —Ñ–∞–π–ª"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIUmGIUdhPo8",
        "outputId": "90c5c51e-dd91-43dc-dc14-1e5a0eeb01de"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('tokenizer.json') as f:\n",
        "    tokenizer_json = json.load(f)\n",
        "\n",
        "tokenizer_json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'added_tokens': [],\n",
              " 'decoder': None,\n",
              " 'model': {'unk_token': 'UNK',\n",
              "  'vocab': {'UNK': 0,\n",
              "   'a': 1,\n",
              "   'dataset': 7,\n",
              "   'from': 5,\n",
              "   'is': 9,\n",
              "   'known': 12,\n",
              "   'list': 2,\n",
              "   'my': 6,\n",
              "   'of': 3,\n",
              "   'sentences': 4,\n",
              "   'text': 10,\n",
              "   'this': 8,\n",
              "   'with': 11,\n",
              "   'words': 13}},\n",
              " 'normalizer': None,\n",
              " 'padding': None,\n",
              " 'post_processor': None,\n",
              " 'pre_tokenizer': {'type': 'Whitespace'},\n",
              " 'truncation': None,\n",
              " 'version': '1.0'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xarM46NohZUs"
      },
      "source": [
        "–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π –∏ —á–∏—Ç–∞–µ–º—ã–π json –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤ —Å–µ–±–µ –ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä `Whitespace`, —Å–ª–æ–≤–∞—Ä—å, —É–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–æ, –∫–∞–∫–æ–π —Ç–æ–∫–µ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ UNK-—Ç–æ–∫–µ–Ω –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ –æ —á—ë–º –º—ã —Å –≤–∞–º–∏ –ø–æ–≥–æ–≤–æ—Ä–∏–º –≤ –±—É–¥—É—â–∏—Ö –∑–∞–Ω—è—Ç–∏—è—Ö.\n",
        "\n",
        "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ `from_file`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmLZD23XhUqS",
        "outputId": "ce5f68cf-4804-4d3a-dd2c-4d929d993e29"
      },
      "source": [
        "loaded_tokenizer = tokenizers.Tokenizer.from_file('tokenizer.json')\n",
        "e = loaded_tokenizer.encode('this is a text')\n",
        "e"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkG6ppi6iBfU",
        "outputId": "86171847-6df4-4f9e-962f-66c5e742fc5c"
      },
      "source": [
        "e.ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 9, 1, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4eRWHJZiNZJ"
      },
      "source": [
        "# Batching\n",
        "\n",
        "–ï—â—ë –æ–¥–Ω–∞ –≤–∞–∂–Ω–∞—è —Ñ–∏—á–∞ ü§ó Tokenizers - —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —ç–Ω–∫–æ–¥–∏—Ç—å –∏ –¥–µ–∫–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ç—Ä—ç–¥–∞—Ö. –≠—Ç–æ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ, —Ç–∫ CPU-bottlenecks (–∫–æ–≥–¥–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–µ –≤—Ä–µ–º—è —Å forward pass –≤–∞—à–µ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏) –æ—á–µ–Ω—å —á–∞—Å—Ç—ã –≤ NLP –∏ –º–æ–≥—É—Ç —Å–∏–ª—å–Ω–æ –∑–∞–º–µ–¥–ª–∏—Ç—å –≤–∞—à—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É, —Ç–∫ GPU –±—É–¥–µ—Ç –∂–¥–∞—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞—Ç—å –≤ —ç—Ç–æ –≤—Ä–µ–º—è.\n",
        "\n",
        "–î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å —ç–Ω–∫–æ–¥–∏–Ω–≥ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å, –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ `encode_batch`. –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ–π, –∫–æ–≥–¥–∞ —Ä–∞–∑–º–µ—Ä –≤–∞—à–µ–≥–æ –±–∞—Ç—á–∞ –±–ª–∏–∑–æ–∫ –∫ 64 –∏–ª–∏ –±–æ–ª—å—à–µ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4So4EDTiJKl",
        "outputId": "9833f683-7e76-41e7-8c79-0e57c746a6be"
      },
      "source": [
        "texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a list of sentences from my dataset', 'this is a text with known words']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLhnX-EGiMg2",
        "outputId": "750ba22a-674d-45e6-cafd-8306e290c66d"
      },
      "source": [
        "batch = tokenizer.encode_batch(texts)\n",
        "batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
              " Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkV-S6VOjeU8",
        "outputId": "7c0f13ab-711d-4d82-e7af-29964c3a72c1"
      },
      "source": [
        "batch[0].ids, batch[1].ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2, 3, 4, 5, 6, 7], [8, 9, 1, 10, 11, 12, 13])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9VM_KlsnciA"
      },
      "source": [
        "–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –±–æ–ª—å—à–µ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å ü§ó Tokenizers, –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –≤–∞–º –ø–æ—á–∏—Ç–∞—Ç—å –∏—Ö [–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é](https://huggingface.co/docs/tokenizers/python/latest). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkUWn-rUmnli"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}