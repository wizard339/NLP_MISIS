{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwELB6DV3AUT4OxosO1dhY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snv-ds/NLP_course/blob/master/week1/lecture_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAsyJjyNt5Iv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpF4iUEstAko"
      },
      "source": [
        "tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9jA2bEmupnP"
      },
      "source": [
        "text = 'Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGrOtDxZs9Wx"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "WordPunctTokenizer().tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1cA45dpud-H"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAP7z8elud7B"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "text_tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "tokenizer.tokenize(text_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl66suXtwGUP"
      },
      "source": [
        "Perform charcter tokenizer using basic Python features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PYinU0avkl1"
      },
      "source": [
        "# YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3fyWbPkwNkq"
      },
      "source": [
        "class CustomCharTokenizer:\n",
        "  def __init___(self):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Ie8fOVwvnX"
      },
      "source": [
        "assert len(CustomCharTokenizer()(text)) == len(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPwoPG2ktKRR"
      },
      "source": [
        "one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-BidndetLvu"
      },
      "source": [
        "text = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
        " Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
        " Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. \n",
        " Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hObc_yMkyaXZ"
      },
      "source": [
        "tokens = set(text.split())\n",
        "list(tokens)[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS2VgTJIyi5I"
      },
      "source": [
        "num2token = dict(enumerate(tokens))\n",
        "token2num = {value:key for key, value in num2token.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD72H5LAyi1K"
      },
      "source": [
        "def create_ohe(word, token2num, vocab_size):\n",
        "  vector = np.zeros((vocab_size, 1))\n",
        "  vector[token2num[word], 0] = 1\n",
        "  return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usGRqXwcyixU"
      },
      "source": [
        "create_ohe('veniam,', token2num, len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GP1Sac_021k"
      },
      "source": [
        "Try to delete punctuation and other tricks to trim vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYrkD40Q12vD"
      },
      "source": [
        "# YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdIoQIH7tMSs"
      },
      "source": [
        "tf-idf usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIEoQkek3hdD"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgIC_AQW3mWF"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mTX-LLB4MFf"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
        "vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN6Zz0dE3-Xn"
      },
      "source": [
        "Initialize and train tf-idf vectors with 1,2 n-grams, and have maximum 30 000 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhosy62-45MV"
      },
      "source": [
        "#YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEerqGX-3yFh"
      },
      "source": [
        "assert vectors.shape == (2034, 30000)\n",
        "assert vectors[0].todense().sum() > 9.6"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}