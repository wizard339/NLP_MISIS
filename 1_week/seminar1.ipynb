{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IL2QzmhC1GkB",
        "z7rm1DEm-1KN",
        "6ltSqTE9_M7f",
        "Ugelc9CwQkBX",
        "kA4Tn1lJ_lgm",
        "n347P2LQAJsr",
        "S6-LNBkiAl7h",
        "CgWhbzBNBWAl",
        "YzqRQbvTCeGs",
        "ppo_wFANDA9n",
        "0XyJGOPBDa28"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptQ5J-qLBTFk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsSz7GKls9x2"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eq1QX6hPy6H"
      },
      "source": [
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuidBaaQQsyp"
      },
      "source": [
        "## simple operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLy0lCVlX713"
      },
      "source": [
        "Now we gone start to work with text. First of all we need to download and simple look at our texts. Lets start from simple preprocessing - lower all text and delete stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ltZrWf2UG91"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MastafaF/multilingual_book_corpus/master/data/harry_potter_ru.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iep2hDrtUG6x"
      },
      "source": [
        "with open('harry_potter_ru.txt', 'r') as f:\n",
        "  hp = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiMSZay7UG3Y"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY1Y2YmcVfXt"
      },
      "source": [
        "hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyaLtAWvYdR1"
      },
      "source": [
        "hp = hp.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHZ70eIoVf4d"
      },
      "source": [
        "Task 1<br>\n",
        "delete from text all stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL2QzmhC1GkB"
      },
      "source": [
        "### help\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2X-hYPP1X6-"
      },
      "source": [
        "# 1. create list\n",
        "# 2. split text on words\n",
        "# 3. check if word in stopwords\n",
        "# 4. return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a1ZeI4-1gY4"
      },
      "source": [
        "### Contine working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLZmrlXYac9z"
      },
      "source": [
        "stop_words = set(stopwords.words(\"russian\"))\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mkMqUdeVfKF"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def delete_stop_words(text: str):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxPPnV9JaOh8"
      },
      "source": [
        "new_hp = delete_stop_words(hp)\n",
        "new_hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEhgppYAVuNO"
      },
      "source": [
        "assert len(new_hp.split()) == 2639"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRlROWhaa97s"
      },
      "source": [
        "Now lets calculate all words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbz7a2q3bCHq"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(hp.split()).most_common(10), Counter(new_hp.split()).most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIe_jqyQb7dm"
      },
      "source": [
        "Task 2<br>\n",
        "Lets delete punctuation from text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7rm1DEm-1KN"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EcE0x99-2s7"
      },
      "source": [
        "# 1. create list\n",
        "# 2. split text on symbols\n",
        "# 3. check if symbol in punctuation\n",
        "# 4. return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgK72Bxi-3PU"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8vtzr4cOSh"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def delete_puckt(text: str, punctuation=punctuation):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D47u0vbPcmxE"
      },
      "source": [
        "new_hp = delete_puckt(new_hp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sstbeF2Rc0LJ"
      },
      "source": [
        "assert len(new_hp) == 18732"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKIAE35UP3UH"
      },
      "source": [
        "## stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wePNq9dAWgCt"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snowball = SnowballStemmer(language=\"russian\")\n",
        "snowball.stem(\"Хороший\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzB2ZvrVc9PG"
      },
      "source": [
        "Task 3<br>\n",
        "Lets do stemming for first 100 words just for practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ltSqTE9_M7f"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Z0JcAg_Orl"
      },
      "source": [
        "# 1. trim first 100 words\n",
        "# 2. iterate over selected words and make stemming of each word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLHNbXEU_Mi1"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP8CErnGf1Pe"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_GDGK6jP6gH"
      },
      "source": [
        "## lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY-fxbeSXExv"
      },
      "source": [
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NEVcFwmXMx8"
      },
      "source": [
        "from pymystem3 import Mystem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDQBL2vATN2i"
      },
      "source": [
        "mystem = Mystem('./mystem')\n",
        "mystem.lemmatize('Хороший')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYvq7PvQUsFw"
      },
      "source": [
        "pip install pymorphy2 > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUoIGHDDWK9i"
      },
      "source": [
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "morph.parse(\"Хороший\")[0].normal_form"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCMv3rNpZZEL"
      },
      "source": [
        "plot words distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFcAYHWtZo-1"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def graph(text: str):\n",
        "    tokens = word_tokenize(text)\n",
        "    fd = FreqDist(tokens)\n",
        "    fd.plot(30,cumulative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVXiUcGzgQcf"
      },
      "source": [
        "graph(new_hp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGPXkqw9d1k-"
      },
      "source": [
        "graph(hp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugelc9CwQkBX"
      },
      "source": [
        "## wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0_EoApuZv9V"
      },
      "source": [
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qpU36s4Zn-j"
      },
      "source": [
        "wordcloud = WordCloud().generate(hp)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy9R1F-qaKRI"
      },
      "source": [
        "wordcloud = WordCloud().generate(new_hp)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av72FovBPuCs"
      },
      "source": [
        "## N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwAol24wgknU"
      },
      "source": [
        "In NLP, you can work not only with individual tokens, but also with groups of tokens that go sequentially. They are named - N-grams, where N - number of sequential tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4eDcZLZgie6"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "tokenize = word_tokenize(new_hp)\n",
        "bigrams = ngrams(tokenize, 2)\n",
        "list(bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwkHGDHSgiS5"
      },
      "source": [
        "trigrams = ngrams(tokenize, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATvD5_77hwp-"
      },
      "source": [
        "Task 4<br>\n",
        "Write on pure python bigrams generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA4Tn1lJ_lgm"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVIuEV9V_n1C"
      },
      "source": [
        "# 1. concatenate each word with next word (zip function can help you)\n",
        "# 2. yield this concatenation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hp6Iesa_lPN"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-aWe6mwh5_D"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def make_bigrams(text: str):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNDt5_MYi5-e"
      },
      "source": [
        "assert len(list(make_bigrams(new_hp))) == len(list(ngrams(tokenize, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_qlCE0kQFjD"
      },
      "source": [
        "# Solving text classification using OHE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnI7j1vlB03h"
      },
      "source": [
        "## data lookup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdujpNoYPpTZ"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3hc8hsxBPXG"
      },
      "source": [
        "df = pd.read_csv('title_conference.csv')\n",
        "df['Conference'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_TZTm1MziN-"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxERWcgLj3v6"
      },
      "source": [
        "df['Conference'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJp6Zleejy_W"
      },
      "source": [
        "Lets plot distribution of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlrW-HyFkEEL"
      },
      "source": [
        "plt.bar(df['Conference'].value_counts().index, df['Conference'].value_counts());"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIq_q7R-zoR4"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Q1pIID3sgd"
      },
      "source": [
        "Let`s look up at some categories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuKhszov3r7y"
      },
      "source": [
        "def category_lookup(df, cat):\n",
        "  return df[df['Conference'] == cat]['Title'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWKIpG8e4AHX"
      },
      "source": [
        "category_lookup(df, 'INFOCOM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuvLeBDLzoKn"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08L2XvL6zoDI"
      },
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(df['Conference'])\n",
        "df['Conference'] = le.transform(df['Conference'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw1MCmE2zIBc"
      },
      "source": [
        "texts = df['Title'].values\n",
        "target = df['Conference'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzqmSQzaB4yg"
      },
      "source": [
        "## lets get to work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU2Hd_UH00Aa"
      },
      "source": [
        "Split train and test data - set test portion as 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajdBhDNOACCK"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "texts_train, texts_test, y_train, y_test = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFQtiRWh1EgB"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))\n",
        "\n",
        "text = 'How to be a grown-up at work: replace \"I don\\'t want to do that\" with \"Ok, great!\".'\n",
        "print(\"before:\", text,)\n",
        "print(\"after:\", preprocess(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUdbldA31WGy"
      },
      "source": [
        "Task 5<br>\n",
        "Tokenize texts for train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n347P2LQAJsr"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpgzT3HzALR9"
      },
      "source": [
        "# 1. you can make it by using functional programming (map(lambda text: ...))\n",
        "# 2. ... - here you need to tokenize and then again join all tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M16Q6n63AJck"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA6vQ4Uj1EX7"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k66Bt5YM1EJL"
      },
      "source": [
        "assert texts_train[5] == 'the treescape system : reuse of pre - computed aggregates over irregular olap hierarchies .'\n",
        "assert texts_test[8] == 'exposing private information by timing web applications .'\n",
        "assert len(texts_test) == len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tsoq7zN4UwG"
      },
      "source": [
        "## Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3wUbq3g4QuN"
      },
      "source": [
        "One traditional approach to such problem is to use bag of words features:\n",
        "1. build a vocabulary of frequent words (use train data only)\n",
        "2. for each training sample, count the number of times a word occurs in it (for each word in vocabulary).\n",
        "3. consider this count a feature for some classifier\n",
        "\n",
        "__Note:__ in practice, you can compute such features using sklearn. __Please don't do that in the current assignment, though.__\n",
        "* `from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVd_B0dZtCPl"
      },
      "source": [
        "Task 6<br>\n",
        "find up to k most frequent tokens in texts_train, sort them by number of occurences (highest first)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6-LNBkiAl7h"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul4EtV1KAlhV"
      },
      "source": [
        "# 1. count all unique words in train texts\n",
        "# 2. use Counter to count all occurences\n",
        "# 3. get top 100 tokens "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ4KfRyvAn3P"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd4qA_xytS58"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REPwle744u2V"
      },
      "source": [
        "Task 7<br>\n",
        "write function for transforming text into BOW (you can forget about OOV words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgWhbzBNBWAl"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNQHk0jhBXnX"
      },
      "source": [
        "# 1. create vector length of vocab\n",
        "# 2. if word in bow_vocab increase by 1 coresponding index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z1DS5JNBVv4"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SvggTz145qU"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXfvpxM94-Gp"
      },
      "source": [
        "X_train_bow = np.stack(list(map(text_to_bow, texts_train)))\n",
        "X_test_bow = np.stack(list(map(text_to_bow, texts_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQOYXHbF498l"
      },
      "source": [
        "k_max = len(set(' '.join(texts_train).split()))\n",
        "assert X_train_bow.shape == (len(texts_train), min(k, k_max))\n",
        "assert X_test_bow.shape == (len(texts_test), min(k, k_max))\n",
        "assert np.all(X_train_bow[5:10].sum(-1) == np.array([len(s.split()) for s in  texts_train[5:10]]))\n",
        "assert len(bow_vocabulary) <= min(k, k_max)\n",
        "assert X_train_bow[6, bow_vocabulary.index('.')] == texts_train[6].split().count('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsU_nyrxsgMU"
      },
      "source": [
        "### trainig and predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnCx5AQBuc-k"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "logit = LogisticRegression(n_jobs=-1, C=1e5, random_state=42)\n",
        "logit.fit(X_train_bow, y_train)\n",
        "y_pred = logit.predict(X_test_bow)\n",
        "\n",
        "print(classification_report(y_test, y_pred,target_names=le.classes_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUNHtllGPuV3"
      },
      "source": [
        "# Solving text classification using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhQJqTVMQNkZ"
      },
      "source": [
        "## Writing TF-IDF from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EqvhRrCvOb7"
      },
      "source": [
        "Not all words are equally useful. One can prioritize rare words and downscale words like \"and\"/\"or\" by using __tf-idf features__. This abbreviation stands for __text frequency/inverse document frequence__ and means exactly that:\n",
        "\n",
        "$$ feature_i = { Count(word_i \\in x) \\times { log {N \\over Count(word_i \\in D) + \\alpha} }}, $$\n",
        "\n",
        "\n",
        "where x is a single text, D is your dataset (a collection of texts), N is a total number of documents and $\\alpha$ is a smoothing hyperparameter (typically 1). \n",
        "And $Count(word_i \\in D)$ is the number of documents where $word_i$ appears.\n",
        "\n",
        "It may also be a good idea to normalize each data sample after computing tf-idf features.\n",
        "\n",
        "__Your task:__ implement tf-idf features, train a model and evaluate weighted f-score. Compare it with basic BagOfWords model from above.\n",
        "\n",
        "__Please don't use sklearn/nltk builtin tf-idf vectorizers in your solution :)__ You can still use 'em for debugging though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z9DHrzkve7f"
      },
      "source": [
        "Blog post about implementing the TF-IDF features from scratch: https://triton.ml/blog/tf-idf-from-scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkhgZUtIvxCt"
      },
      "source": [
        "Task 8<br>\n",
        "Compute TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_7_kIc3v1pR"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def compute_idf(doc):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzqRQbvTCeGs"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41FaOyiiCepX"
      },
      "source": [
        "# 1. create dict\n",
        "# 2. count occurence of each word in document\n",
        "# 3. divide number of occurence by length of doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsmUPKn1Cd9A"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wFZeeAfZNlH"
      },
      "source": [
        "def compute_count_dict(data):\n",
        "  count_dict = {}\n",
        "  for text in data:\n",
        "    for word in set(text.split()):\n",
        "      if word in count_dict:\n",
        "        count_dict[word] += 1\n",
        "      else:\n",
        "        count_dict[word] = 1\n",
        "  return count_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMej8ML_y9n1"
      },
      "source": [
        "Task 9<br>\n",
        "fill code to compute IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppo_wFANDA9n"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTTk_Uz3DCrG"
      },
      "source": [
        "# 1. for each word in count_dict calculate idf (look at the formula)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJztxGA0DAyQ"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0a_vwaUyIx1"
      },
      "source": [
        "from math import log\n",
        "\n",
        "def compute_idf(data, alpha=1):\n",
        "  count_dict = compute_count_dict(data)\n",
        "  idf_dict = {}\n",
        "  # YOUR CODE HERE\n",
        "  return idf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTZbM3X1yUuE"
      },
      "source": [
        "idf_dict = compute_idf(texts_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awP6hV1nyaM9"
      },
      "source": [
        "def compute_tf_idf(text, data=texts_train, idf_dict=None):\n",
        "  if idf_dict is None:\n",
        "    idf_dict = compute_idf(data)\n",
        "  tf_dict = compute_tf(text)\n",
        "  for word in tf_dict.keys():\n",
        "    if word in word in idf_dict.keys():\n",
        "      tf_dict[word] *= idf_dict[word]\n",
        "    else:\n",
        "      tf_dict[word] = 0\n",
        "  return tf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc_9W4Lsy7vO"
      },
      "source": [
        "Task 10<br>\n",
        "fill code for filling tf-idf vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XyJGOPBDa28"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO1hqGirDgIG"
      },
      "source": [
        "# 1. if word in idf_dict\n",
        "# 2. get it index in all_words\n",
        "# 3. initialize corresponding component in tf_idf vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdIDnL7CDaoJ"
      },
      "source": [
        "### continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TESYaNpjzG2j"
      },
      "source": [
        "all_words = sorted(idf_dict.keys())\n",
        "\n",
        "def text_to_tfidf(text, idf_dict=idf_dict, all_words=all_words):\n",
        "  tf_idf = [0] * len(idf_dict)\n",
        "  tf_idf_dict = compute_tf_idf(text, idf_dict=idf_dict)\n",
        "  for word in text.split():\n",
        "    # YOUR CODE HERE\n",
        "  return np.array(tf_idf, 'float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAV7TNS06Yk7"
      },
      "source": [
        "X_train_tf = np.stack(list(map(text_to_tfidf, texts_train)))\n",
        "X_test_tf = np.stack(list(map(text_to_tfidf, texts_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmBk7qh_9Ch7"
      },
      "source": [
        "### Evaluate TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILCHKAkY8ovs"
      },
      "source": [
        "logit = LogisticRegression(n_jobs=-1, C=1e5, random_state=42)\n",
        "logit.fit(X_train_tf, y_train)\n",
        "y_pred = logit.predict(X_test_tf)\n",
        "\n",
        "print(classification_report(y_test, y_pred,target_names=le.classes_))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
