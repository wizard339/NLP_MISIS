{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar1_solved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6eq1QX6hPy6H",
        "RuidBaaQQsyp",
        "IL2QzmhC1GkB",
        "1a1ZeI4-1gY4",
        "z7rm1DEm-1KN",
        "ZgK72Bxi-3PU",
        "JKIAE35UP3UH",
        "6ltSqTE9_M7f",
        "sLHNbXEU_Mi1",
        "0_GDGK6jP6gH",
        "Ugelc9CwQkBX",
        "av72FovBPuCs",
        "kA4Tn1lJ_lgm",
        "8hp6Iesa_lPN",
        "n347P2LQAJsr",
        "M16Q6n63AJck",
        "S6-LNBkiAl7h",
        "ppo_wFANDA9n",
        "0XyJGOPBDa28"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptQ5J-qLBTFk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsSz7GKls9x2"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eq1QX6hPy6H"
      },
      "source": [
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuidBaaQQsyp"
      },
      "source": [
        "## simple operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLy0lCVlX713"
      },
      "source": [
        "Now we gone start to work with text. First of all we need to download and simple look at our texts. Lets start from simple preprocessing - lower all text and delete stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ltZrWf2UG91"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MastafaF/multilingual_book_corpus/master/data/harry_potter_ru.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iep2hDrtUG6x"
      },
      "source": [
        "with open('harry_potter_ru.txt', 'r') as f:\n",
        "  hp = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiMSZay7UG3Y"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY1Y2YmcVfXt"
      },
      "source": [
        "hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyaLtAWvYdR1"
      },
      "source": [
        "hp = hp.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHZ70eIoVf4d"
      },
      "source": [
        "Task 1<br>\n",
        "delete from text all stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL2QzmhC1GkB"
      },
      "source": [
        "### help\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2X-hYPP1X6-"
      },
      "source": [
        "# 1. create list\n",
        "# 2. split text on words\n",
        "# 3. check if word in stopwords\n",
        "# 4. return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a1ZeI4-1gY4"
      },
      "source": [
        "### Contine working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLZmrlXYac9z"
      },
      "source": [
        "stop_words = set(stopwords.words(\"russian\"))\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mkMqUdeVfKF"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def delete_stop_words(text: str):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6qpc2LVYwlA"
      },
      "source": [
        "def delete_stop_words(text: str, stopwords=stop_words):\n",
        "  result = list()\n",
        "  for word in text.split():\n",
        "    if word.lower() in stop_words:\n",
        "      continue\n",
        "    result.append(word)\n",
        "  return ' '.join(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxPPnV9JaOh8"
      },
      "source": [
        "new_hp = delete_stop_words(hp)\n",
        "new_hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEhgppYAVuNO"
      },
      "source": [
        "assert len(new_hp.split()) == 2639"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRlROWhaa97s"
      },
      "source": [
        "Now lets calculate all words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbz7a2q3bCHq"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(hp.split()).most_common(10), Counter(new_hp.split()).most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIe_jqyQb7dm"
      },
      "source": [
        "Task 2<br>\n",
        "Lets delete punctuation from text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7rm1DEm-1KN"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EcE0x99-2s7"
      },
      "source": [
        "# 1. create list\n",
        "# 2. split text on symbols\n",
        "# 3. check if symbol in punctuation\n",
        "# 4. return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgK72Bxi-3PU"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8vtzr4cOSh"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def delete_puckt(text: str, punctuation=punctuation):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uZEEEFVb6Zg"
      },
      "source": [
        "def delete_puckt(text: str, punctuation=punctuation):\n",
        "  result = list()\n",
        "  for char in text:\n",
        "    if char in punctuation:\n",
        "      continue\n",
        "    result.append(char)\n",
        "  return ''.join(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D47u0vbPcmxE"
      },
      "source": [
        "new_hp = delete_puckt(new_hp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sstbeF2Rc0LJ"
      },
      "source": [
        "assert len(new_hp) == 18732"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKIAE35UP3UH"
      },
      "source": [
        "## stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "wePNq9dAWgCt",
        "outputId": "d15de68b-05fa-40d2-cbd8-2b9710413f75"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snowball = SnowballStemmer(language=\"russian\")\n",
        "snowball.stem(\"Хороший\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'хорош'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzB2ZvrVc9PG"
      },
      "source": [
        "Task 3<br>\n",
        "Lets do stemming for first 100 words just for practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ltSqTE9_M7f"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Z0JcAg_Orl"
      },
      "source": [
        "# 1. trim first 100 words\n",
        "# 2. iterate over selected words and make stemming of each word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLHNbXEU_Mi1"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP8CErnGf1Pe"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u39hVEBydDyF"
      },
      "source": [
        "for word in new_hp.split()[:100]:\n",
        "  print(snowball.stem(word), end='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_GDGK6jP6gH"
      },
      "source": [
        "## lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY-fxbeSXExv"
      },
      "source": [
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NEVcFwmXMx8"
      },
      "source": [
        "from pymystem3 import Mystem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDQBL2vATN2i",
        "outputId": "2e9588b5-18d7-4eee-f134-0bbdb1b8183b"
      },
      "source": [
        "mystem = Mystem('./mystem')\n",
        "mystem.lemmatize('Хороший')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['хороший', '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYvq7PvQUsFw"
      },
      "source": [
        "pip install pymorphy2 > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "EUoIGHDDWK9i",
        "outputId": "f36a2734-3655-4340-f28a-e905e3470c2a"
      },
      "source": [
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "morph.parse(\"Хороший\")[0].normal_form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'хороший'"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCMv3rNpZZEL"
      },
      "source": [
        "plot words distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFcAYHWtZo-1"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def graph(text: str):\n",
        "    tokens = word_tokenize(text)\n",
        "    fd = FreqDist(tokens)\n",
        "    fd.plot(30,cumulative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVXiUcGzgQcf"
      },
      "source": [
        "graph(new_hp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGPXkqw9d1k-"
      },
      "source": [
        "graph(hp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugelc9CwQkBX"
      },
      "source": [
        "## wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0_EoApuZv9V"
      },
      "source": [
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qpU36s4Zn-j"
      },
      "source": [
        "wordcloud = WordCloud().generate(hp)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy9R1F-qaKRI"
      },
      "source": [
        "wordcloud = WordCloud().generate(new_hp)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av72FovBPuCs"
      },
      "source": [
        "## N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwAol24wgknU"
      },
      "source": [
        "In NLP, you can work not only with individual tokens, but also with groups of tokens that go sequentially. They are named - N-grams, where N - number of sequential tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4eDcZLZgie6"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "tokenize = word_tokenize(new_hp)\n",
        "bigrams = ngrams(tokenize, 2)\n",
        "list(bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwkHGDHSgiS5"
      },
      "source": [
        "trigrams = ngrams(tokenize, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATvD5_77hwp-"
      },
      "source": [
        "Task 4<br>\n",
        "Write on pure python bigrams generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA4Tn1lJ_lgm"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVIuEV9V_n1C"
      },
      "source": [
        "# 1. concatenate each word with next word (zip function can help you)\n",
        "# 2. yield this concatenation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hp6Iesa_lPN"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-aWe6mwh5_D"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def make_bigrams(text: str):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4KdU4ehh51k"
      },
      "source": [
        "def make_bigrams(text: str):\n",
        "  for bigram in zip(text.split(), text.split()[1:]):\n",
        "    yield bigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNDt5_MYi5-e"
      },
      "source": [
        "assert len(list(make_bigrams(new_hp))) == len(list(ngrams(tokenize, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_qlCE0kQFjD"
      },
      "source": [
        "# Solving text classification using OHE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnI7j1vlB03h"
      },
      "source": [
        "## data lookup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdujpNoYPpTZ"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3hc8hsxBPXG",
        "outputId": "c93cae05-ef0f-4c27-ab2a-1eaa8ade9c08"
      },
      "source": [
        "df = pd.read_csv('title_conference.csv')\n",
        "df['Conference'].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_TZTm1MziN-",
        "outputId": "e10709dd-5bcf-421b-c4fb-fd895e731e62"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2507 entries, 0 to 2506\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Title       2507 non-null   object\n",
            " 1   Conference  2507 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 39.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxERWcgLj3v6",
        "outputId": "92249fee-55be-4cc8-f355-d752dd385bb4"
      },
      "source": [
        "df['Conference'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ISCAS       864\n",
              "INFOCOM     515\n",
              "VLDB        423\n",
              "WWW         379\n",
              "SIGGRAPH    326\n",
              "Name: Conference, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJp6Zleejy_W"
      },
      "source": [
        "Lets plot distribution of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JlrW-HyFkEEL",
        "outputId": "3dbdd060-6c20-4d59-c400-a66ea8e5ca24"
      },
      "source": [
        "plt.bar(df['Conference'].value_counts().index, df['Conference'].value_counts());"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "HIq_q7R-zoR4",
        "outputId": "d360accb-118e-4cac-c67f-2f0ba913fb3b"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Conference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Innovation in Database Management: Computer Sc...</td>\n",
              "      <td>VLDB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>High performance prime field multiplication fo...</td>\n",
              "      <td>ISCAS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>enchanted scissors: a scissor interface for su...</td>\n",
              "      <td>SIGGRAPH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Detection of channel degradation attack by Int...</td>\n",
              "      <td>INFOCOM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pinning a Complex Network through the Betweenn...</td>\n",
              "      <td>ISCAS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title Conference\n",
              "0  Innovation in Database Management: Computer Sc...       VLDB\n",
              "1  High performance prime field multiplication fo...      ISCAS\n",
              "2  enchanted scissors: a scissor interface for su...   SIGGRAPH\n",
              "3  Detection of channel degradation attack by Int...    INFOCOM\n",
              "4  Pinning a Complex Network through the Betweenn...      ISCAS"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Q1pIID3sgd"
      },
      "source": [
        "Let`s look up at some categories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuKhszov3r7y"
      },
      "source": [
        "def category_lookup(df, cat):\n",
        "  return df[df['Conference'] == cat]['Title'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWKIpG8e4AHX"
      },
      "source": [
        "category_lookup(df, 'INFOCOM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuvLeBDLzoKn"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "08L2XvL6zoDI",
        "outputId": "8888868c-a269-4ca8-c282-f35619c6679f"
      },
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(df['Conference'])\n",
        "df['Conference'] = le.transform(df['Conference'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Conference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Innovation in Database Management: Computer Sc...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>High performance prime field multiplication fo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>enchanted scissors: a scissor interface for su...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Detection of channel degradation attack by Int...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pinning a Complex Network through the Betweenn...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title  Conference\n",
              "0  Innovation in Database Management: Computer Sc...           3\n",
              "1  High performance prime field multiplication fo...           1\n",
              "2  enchanted scissors: a scissor interface for su...           2\n",
              "3  Detection of channel degradation attack by Int...           0\n",
              "4  Pinning a Complex Network through the Betweenn...           1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw1MCmE2zIBc"
      },
      "source": [
        "texts = df['Title'].values\n",
        "target = df['Conference'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzqmSQzaB4yg"
      },
      "source": [
        "## lets get to work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU2Hd_UH00Aa"
      },
      "source": [
        "Split train and test data - set test portion as 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajdBhDNOACCK"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "texts_train, texts_test, y_train, y_test = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8USXIwVczH2b"
      },
      "source": [
        "texts_train, texts_test, y_train, y_test = train_test_split(texts, target,\n",
        "                                                            test_size=0.25,\n",
        "                                                            random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFQtiRWh1EgB",
        "outputId": "09b231fa-ea88-4ff3-bdd2-b76142266470"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))\n",
        "\n",
        "text = 'How to be a grown-up at work: replace \"I don\\'t want to do that\" with \"Ok, great!\".'\n",
        "print(\"before:\", text,)\n",
        "print(\"after:\", preprocess(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: How to be a grown-up at work: replace \"I don't want to do that\" with \"Ok, great!\".\n",
            "after: how to be a grown - up at work : replace \" i don ' t want to do that \" with \" ok , great !\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUdbldA31WGy"
      },
      "source": [
        "Task 5<br>\n",
        "Preprocess texts for train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n347P2LQAJsr"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpgzT3HzALR9"
      },
      "source": [
        "# 1. you can make it by using functional programming (map(lambda text: ...))\n",
        "# 2. ... - here you need to tokenize and then again join all tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M16Q6n63AJck"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA6vQ4Uj1EX7"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq7b6ArC1EQW"
      },
      "source": [
        "texts_train = np.array(list(map(lambda text: ' '.join(tokenizer.tokenize(text.lower())), texts_train)))\n",
        "texts_test = np.array(list(map(lambda text: ' '.join(tokenizer.tokenize(text.lower())), texts_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k66Bt5YM1EJL"
      },
      "source": [
        "assert texts_train[5] == 'the treescape system : reuse of pre - computed aggregates over irregular olap hierarchies .'\n",
        "assert texts_test[8] == 'exposing private information by timing web applications .'\n",
        "assert len(texts_test) == len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tsoq7zN4UwG"
      },
      "source": [
        "## Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3wUbq3g4QuN"
      },
      "source": [
        "One traditional approach to such problem is to use bag of words features:\n",
        "1. build a vocabulary of frequent words (use train data only)\n",
        "2. for each training sample, count the number of times a word occurs in it (for each word in vocabulary).\n",
        "3. consider this count a feature for some classifier\n",
        "\n",
        "__Note:__ in practice, you can compute such features using sklearn. __Please don't do that in the current assignment, though.__\n",
        "* `from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVd_B0dZtCPl"
      },
      "source": [
        "Task 6<br>\n",
        "find up to k most frequent tokens in texts_train, sort them by number of occurences (highest first)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6-LNBkiAl7h"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul4EtV1KAlhV"
      },
      "source": [
        "# 1. count all unique words in train texts\n",
        "# 2. use Counter to count all occurences\n",
        "# 3. get top 100 tokens "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ4KfRyvAn3P"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd4qA_xytS58"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZs6hKGQ2IAI",
        "outputId": "bcf2360c-bee0-44ff-c38f-472a590889d3"
      },
      "source": [
        "k = min(10000, len(set(' '.join(texts_train).split())))\n",
        "\n",
        "counter = Counter(' '.join(texts_train).split())\n",
        "\n",
        "bow_vocabulary = list(map(lambda x: x[0], counter.most_common()))\n",
        "\n",
        "print('example features:', bow_vocabulary[::100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example features: ['.', 'packet', 'cross', 'similarity', 'fluid', 'sram', 'finding', 'supporting', 'next', 'algorithmic', 'smooth', 'width', 'gossip', 'training', 'conditions', 'mm', 'special', 'elastic', 'rasmalai', 'cmut', 'rise', 'archives', 'tweets', 'multiview', 'genomics', 'nyi', 'leashes', 'ambiguity', 'mirrors', 'bicubic', 'dtmos', 'benchmark', 'hexahedral', 'putting', 'historical', 'rigid', 'outside', 'white', 'compiling', 'uncoded', 'advertiser', 'xquery']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REPwle744u2V"
      },
      "source": [
        "Task 7<br>\n",
        "write function for transforming text into BOW (you can forget about OOV words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgWhbzBNBWAl"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNQHk0jhBXnX"
      },
      "source": [
        "# 1. create vector length of vocab\n",
        "# 2. if word in bow_vocab increase by 1 coresponding index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z1DS5JNBVv4"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SvggTz145qU"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zteos1q62HwF"
      },
      "source": [
        "def text_to_bow(text):\n",
        "    \"\"\" convert text string to an array of token counts. Use bow_vocabulary. \"\"\"\n",
        "    ohe = [0] * len(bow_vocabulary)\n",
        "    for word in text.split():\n",
        "      if word in bow_vocabulary:\n",
        "        i = bow_vocabulary.index(word)\n",
        "        ohe[i] += 1\n",
        "    return np.array(ohe, 'float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXfvpxM94-Gp"
      },
      "source": [
        "X_train_bow = np.stack(list(map(text_to_bow, texts_train)))\n",
        "X_test_bow = np.stack(list(map(text_to_bow, texts_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQOYXHbF498l"
      },
      "source": [
        "k_max = len(set(' '.join(texts_train).split()))\n",
        "assert X_train_bow.shape == (len(texts_train), min(k, k_max))\n",
        "assert X_test_bow.shape == (len(texts_test), min(k, k_max))\n",
        "assert np.all(X_train_bow[5:10].sum(-1) == np.array([len(s.split()) for s in  texts_train[5:10]]))\n",
        "assert len(bow_vocabulary) <= min(k, k_max)\n",
        "assert X_train_bow[6, bow_vocabulary.index('.')] == texts_train[6].split().count('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsU_nyrxsgMU"
      },
      "source": [
        "### trainig and predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnCx5AQBuc-k",
        "outputId": "80f3a93a-8341-4b6b-d981-b294ea8dff83"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "logit = LogisticRegression(n_jobs=-1, C=1e5, random_state=42)\n",
        "logit.fit(X_train_bow, y_train)\n",
        "y_pred = logit.predict(X_test_bow)\n",
        "\n",
        "print(classification_report(y_test, y_pred,target_names=le.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     INFOCOM       0.79      0.76      0.78       123\n",
            "       ISCAS       0.87      0.81      0.84       226\n",
            "    SIGGRAPH       0.63      0.70      0.67        84\n",
            "        VLDB       0.61      0.64      0.63       105\n",
            "         WWW       0.63      0.67      0.65        89\n",
            "\n",
            "    accuracy                           0.74       627\n",
            "   macro avg       0.71      0.72      0.71       627\n",
            "weighted avg       0.75      0.74      0.74       627\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUNHtllGPuV3"
      },
      "source": [
        "# Solving text classification using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhQJqTVMQNkZ"
      },
      "source": [
        "## Writing TF-IDF from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EqvhRrCvOb7"
      },
      "source": [
        "Not all words are equally useful. One can prioritize rare words and downscale words like \"and\"/\"or\" by using __tf-idf features__. This abbreviation stands for __text frequency/inverse document frequence__ and means exactly that:\n",
        "\n",
        "$$ feature_i = { Count(word_i \\in x) \\times { log {N \\over Count(word_i \\in D) + \\alpha} }}, $$\n",
        "\n",
        "\n",
        "where x is a single text, D is your dataset (a collection of texts), N is a total number of documents and $\\alpha$ is a smoothing hyperparameter (typically 1). \n",
        "And $Count(word_i \\in D)$ is the number of documents where $word_i$ appears.\n",
        "\n",
        "It may also be a good idea to normalize each data sample after computing tf-idf features.\n",
        "\n",
        "__Your task:__ implement tf-idf features, train a model and evaluate weighted f-score. Compare it with basic BagOfWords model from above.\n",
        "\n",
        "__Please don't use sklearn/nltk builtin tf-idf vectorizers in your solution :)__ You can still use 'em for debugging though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z9DHrzkve7f"
      },
      "source": [
        "Blog post about implementing the TF-IDF features from scratch: https://triton.ml/blog/tf-idf-from-scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkhgZUtIvxCt"
      },
      "source": [
        "Task 8<br>\n",
        "Compute TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_7_kIc3v1pR"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzqRQbvTCeGs"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41FaOyiiCepX"
      },
      "source": [
        "# 1. create dict\n",
        "# 2. count occurence of each word in document\n",
        "# 3. divide number of occurence by length of doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsmUPKn1Cd9A"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2gkiVnaPu4k"
      },
      "source": [
        "def compute_tf(doc):\n",
        "  tf_dict = {}\n",
        "  for word in doc.split():\n",
        "    if word in tf_dict.keys():\n",
        "      tf_dict[word] += 1\n",
        "    else:\n",
        "      tf_dict[word] = 1\n",
        "  for word in tf_dict.keys():\n",
        "    tf_dict[word] /= len(doc)\n",
        "  return tf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wFZeeAfZNlH"
      },
      "source": [
        "def compute_count_dict(data):\n",
        "  count_dict = {}\n",
        "  for text in data:\n",
        "    for word in set(text.split()):\n",
        "      if word in count_dict:\n",
        "        count_dict[word] += 1\n",
        "      else:\n",
        "        count_dict[word] = 1\n",
        "  return count_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMej8ML_y9n1"
      },
      "source": [
        "Task 9<br>\n",
        "fill code to compute IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppo_wFANDA9n"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTTk_Uz3DCrG"
      },
      "source": [
        "# 1. for each word in count_dict calculate idf (look at the formula)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJztxGA0DAyQ"
      },
      "source": [
        "### Continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0a_vwaUyIx1"
      },
      "source": [
        "from math import log\n",
        "\n",
        "def compute_idf(data, alpha=1):\n",
        "  count_dict = compute_count_dict(data)\n",
        "  idf_dict = {}\n",
        "  # YOUR CODE HERE\n",
        "  return idf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0CmvsaPxmII"
      },
      "source": [
        "from math import log\n",
        "\n",
        "def compute_idf(data, alpha=1):\n",
        "  count_dict = compute_count_dict(data)\n",
        "  idf_dict = {}\n",
        "  for word in count_dict:\n",
        "    idf_dict[word] = log(len(count_dict) / (count_dict[word] + alpha))\n",
        "  return idf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTZbM3X1yUuE"
      },
      "source": [
        "idf_dict = compute_idf(texts_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awP6hV1nyaM9"
      },
      "source": [
        "def compute_tf_idf(text, data=texts_train, idf_dict=None):\n",
        "  if idf_dict is None:\n",
        "    idf_dict = compute_idf(data)\n",
        "  tf_dict = compute_tf(text)\n",
        "  for word in tf_dict.keys():\n",
        "    if word in word in idf_dict.keys():\n",
        "      tf_dict[word] *= idf_dict[word]\n",
        "    else:\n",
        "      tf_dict[word] = 0\n",
        "  return tf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc_9W4Lsy7vO"
      },
      "source": [
        "Task 10<br>\n",
        "fill code for filling tf-idf vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XyJGOPBDa28"
      },
      "source": [
        "### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO1hqGirDgIG"
      },
      "source": [
        "# 1. if word in idf_dict\n",
        "# 2. get it index in all_words\n",
        "# 3. initialize corresponding component in tf_idf vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdIDnL7CDaoJ"
      },
      "source": [
        "### continue working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TESYaNpjzG2j"
      },
      "source": [
        "all_words = sorted(idf_dict.keys())\n",
        "\n",
        "def text_to_tfidf(text, idf_dict=idf_dict, all_words=all_words):\n",
        "  tf_idf = [0] * len(idf_dict)\n",
        "  tf_idf_dict = compute_tf_idf(text, idf_dict=idf_dict)\n",
        "  for word in text.split():\n",
        "    # YOUR CODE HERE\n",
        "  return np.array(tf_idf, 'float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgXmXahoyjHs"
      },
      "source": [
        "all_words = sorted(idf_dict.keys())\n",
        "\n",
        "def text_to_tfidf(text, idf_dict=idf_dict, all_words=all_words):\n",
        "  tf_idf = [0] * len(idf_dict)\n",
        "  tf_idf_dict = compute_tf_idf(text, idf_dict=idf_dict)\n",
        "  for word in text.split():\n",
        "    if word in idf_dict.keys():\n",
        "      i = all_words.index(word)\n",
        "      tf_idf[i] = tf_idf_dict[word]\n",
        "  return np.array(tf_idf, 'float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAV7TNS06Yk7"
      },
      "source": [
        "X_train_tf = np.stack(list(map(text_to_tfidf, texts_train)))\n",
        "X_test_tf = np.stack(list(map(text_to_tfidf, texts_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmBk7qh_9Ch7"
      },
      "source": [
        "### Evaluate TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILCHKAkY8ovs",
        "outputId": "14b9c20c-0cef-4098-aa35-cfa1069d4d6c"
      },
      "source": [
        "logit = LogisticRegression(n_jobs=-1, C=1e5, random_state=42)\n",
        "logit.fit(X_train_tf, y_train)\n",
        "y_pred = logit.predict(X_test_tf)\n",
        "\n",
        "print(classification_report(y_test, y_pred,target_names=le.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     INFOCOM       0.80      0.80      0.80       123\n",
            "       ISCAS       0.85      0.90      0.87       226\n",
            "    SIGGRAPH       0.84      0.61      0.70        84\n",
            "        VLDB       0.67      0.67      0.67       105\n",
            "         WWW       0.64      0.71      0.67        89\n",
            "\n",
            "    accuracy                           0.78       627\n",
            "   macro avg       0.76      0.74      0.74       627\n",
            "weighted avg       0.78      0.78      0.77       627\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA3Zkhk590x_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}